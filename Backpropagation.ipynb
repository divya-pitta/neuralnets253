{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "\n",
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "mndata = MNIST(\"/Users/jigyayadav/Desktop/Codes/neuralnets253/HW1\")\n",
    "mndata.gz = True\n",
    "images, labels = mndata.load_training() #Images is a list of 60000 images of 784 dimensions, Labels is a list of 60000 ints\n",
    "imagesTest, labelsTest = mndata.load_testing()\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "imagesTest = np.array(imagesTest)\n",
    "labelsTest = np.array(labelsTest)\n",
    "\n",
    "images_train = images[:200]\n",
    "images_test = imagesTest[-50:]\n",
    "labels_train = labels[:200]\n",
    "labels_test = labelsTest[-50:]\n",
    "images_train = np.array(images_train)\n",
    "labels_train = np.array(labels_train)\n",
    "images_test = np.array(images_test)\n",
    "labels_test = np.array(labels_test)\n",
    "\n",
    "# Normalization\n",
    "images_train = images_train*(1/127.5)\n",
    "images_test = images_test*(1/127.5)\n",
    "arrOnes = np.ones_like(images_train)\n",
    "images_train = images_train-arrOnes\n",
    "arrOnes = np.ones_like(images_test)\n",
    "images_test = images_test-arrOnes\n",
    "\n",
    "# Should this be appended after or before normalization\n",
    "images_train = np.insert(images_train, 0, 1, axis=1)\n",
    "images_test = np.insert(images_test, 0, 1, axis=1)\n",
    "\n",
    "numFeatures = len(images[0, :])\n",
    "\n",
    "# Divide between validation and training\n",
    "from sklearn.model_selection import train_test_split\n",
    "images_train, images_validation, labels_train, labels_validation = train_test_split(images_train, labels_train, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelNums = 10\n",
    "\n",
    "def labelsToVectForm(labels):\n",
    "    vectForm = np.zeros((len(labels), labelNums))\n",
    "    for i in range(len(labels)):\n",
    "        vectForm[i][labels[i]] = 1\n",
    "    return vectForm\n",
    "\n",
    "labels_train = labelsToVectForm(labels_train)\n",
    "labels_test = labelsToVectForm(labels_test)\n",
    "labels_validation = labelsToVectForm(labels_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sigmoid activation\n",
      "(785, 64)\n",
      "(180, 785)\n",
      "For input to hidden layers\n",
      "In sigmoid activation\n",
      "(785, 64)\n",
      "(180, 785)\n",
      "In sigmoid activation\n",
      "(785, 64)\n",
      "(180, 785)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d4dbd5ca1b8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradientUpdateSingleUpdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearningRate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m \u001b[0mcheckGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweightsIJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweightsJK\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0mweightsIJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m785\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0mweightsJK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m65\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-d4dbd5ca1b8a>\u001b[0m in \u001b[0;36mcheckGradient\u001b[0;34m(weights, epsilon, x, t)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0msummation_k\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mactualDerivative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputLayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0moutputLayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummation_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                 \u001b[0mapproximateDerivative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculateApproximateDerivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactualDerivative\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mapproximateDerivative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Difference between approximate gradient = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-d4dbd5ca1b8a>\u001b[0m in \u001b[0;36mcalculateApproximateDerivative\u001b[0;34m(weights, epsilon, x, y, t, i, j, n, p)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mnewWeights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mcostFunction2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcostFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewWeights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mapproximateDerivative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcostFunction1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcostFunction2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mapproximateDerivative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Calculate things going forward\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "numInputLayers = 784\n",
    "numHiddenLayers = 64\n",
    "numOutputLayers = 10\n",
    "numExamples = 100\n",
    "\n",
    "# X is (n*i) and weights is (i*j)\n",
    "# returns (n*j) vector\n",
    "def calculateWeightedInput(weights, x):\n",
    "    return (x.dot(weights))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def softmaxFunc(prod):\n",
    "#     for i in range(len(prod)):\n",
    "#         for j in range(len(prod[i, :])):\n",
    "#             prod[i][j] = math.exp(prod[i][j])\n",
    "    prod = np.exp(prod)\n",
    "    for i in range(len(prod)):\n",
    "        rowSum = sum(prod[i, :])\n",
    "        for j in range(len(prod[i, :])):\n",
    "            prod[i][j] = prod[i][j]/rowSum    \n",
    "    return prod\n",
    "\n",
    "# X is (n*i) and weights is (i*j)\n",
    "# returns (n*j) vector\n",
    "def sigmoidActivation(weights, x):\n",
    "    print(\"In sigmoid activation\")\n",
    "    print(weights.shape)\n",
    "    print(x.shape)\n",
    "    prod = calculateWeightedInput(weights, x)\n",
    "    for i in range(len(prod)):\n",
    "        for j in range(len(prod[i, :])):\n",
    "            prod[i][j] = sigmoid(prod[i][j])\n",
    "    return prod\n",
    "\n",
    "# X is (n*i) and weights is (i*j)\n",
    "# returns (n*j) vector\n",
    "def softmaxActivation(weights, x):\n",
    "    prod = calculateWeightedInput(weights, x)\n",
    "    prod = softmaxFunc(prod)\n",
    "    return prod\n",
    "\n",
    "# X is (n*i) and weights is (i*j)\n",
    "# returns (n*j) vector\n",
    "def YHotEncoding(a):\n",
    "    b = np.zeros_like(a)\n",
    "    b[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    return b\n",
    "\n",
    "def calculateOutputPerLayer(weights, x):\n",
    "    outputHiddenLayer = sigmoidActivation(weights[0], x)\n",
    "    inputSecondLayer = np.insert(outputHiddenLayer, 0, 1, axis=1)\n",
    "    outputFinalLayer = softmaxActivation(weights[1], inputSecondLayer)\n",
    "#     outputFinalLayer = findYHotEncoding(outputFinalLayer)\n",
    "    return [outputHiddenLayer, outputFinalLayer]\n",
    "\n",
    "# Calculate the derivative of the cost function wrt hidden to output layer weights\n",
    "def delEWjk(x, y, t, z):\n",
    "    differencePredictedActual = (y-t)\n",
    "    # differentiation is j*k array\n",
    "    differentiation = z.T.dot(differencePredictedActual)\n",
    "    differentiation = differentiation*(1/len(x))\n",
    "    return differentiation\n",
    "\n",
    "# Calculate the derivative of the cost function wrt input to hidden layer weights\n",
    "def delEWij(x, y, t, g, weights):\n",
    "    differencePredictedActual = (y-t)\n",
    "    # summation_k is n*j\n",
    "    summation_k = (differencePredictedActual.dot(weights.T))\n",
    "    # calculate g(a_j)*(1-g(a_j))- n*j\n",
    "    # g_differentiation is (n*j)\n",
    "    g_differentiation = np.multiply(g, np.ones_like(g)-g)\n",
    "    prod1 = np.multiply(g_differentiation, summation_k)\n",
    "    prod2 = x.T.dot(prod1)\n",
    "    prod2 = prod2*(1/len(x))\n",
    "    return prod2\n",
    "\n",
    "# x is n*i vector\n",
    "# weights is j*k vector\n",
    "# y and t are (n*k) vector\n",
    "# z is (n*j) vector\n",
    "def updateWeightsHiddenToOutput(weights, x, y, t, z, learningRate):\n",
    "    derivativeE = delEWjk(x, y, t, z)\n",
    "    updatedWeights = weights - learningRate*derivativeE\n",
    "    return updatedWeights\n",
    "\n",
    "# x is (n*i) vector\n",
    "# y, t is (n*k)\n",
    "# g is (n*j) vector\n",
    "# weights is (j*k)\n",
    "def updateWeightsInputToHidden(weights, x, y, t, g, learningRate):\n",
    "    derivativeE = delEWij(x, y, t, g, weights)\n",
    "    updatedWeights = weights - learningRate*derivativeE\n",
    "    return updatedWeights\n",
    "\n",
    "def costFunction(x, t, weights):\n",
    "    sampLen = len(x)\n",
    "    predProb = calculateOutputPerLayer(weights, x)[1]\n",
    "#     vectorLabels = labelsToVectForm(t)\n",
    "    cost = -1*(np.multiply(t, np.log(predProb)))/sampLen\n",
    "    return [np.sum(cost), cost]\n",
    "\n",
    "# Calculate derivate of nth example wrt weight_ij\n",
    "def calculateApproximateDerivative(weights, epsilon, x, y, t, i, j, n, p):\n",
    "    newWeights = weights\n",
    "    newWeights[p][i][j] += epsilon\n",
    "    costFunction1 = costFunction(x, t, newWeights)\n",
    "    newWeights = weights\n",
    "    newWeights[p][i][j] -= epsilon\n",
    "    costFunction2 = costFunction(x, t, newWeights)\n",
    "    approximateDerivative = (costFunction1[n]-costFunction2[n])/(2*epsilon)\n",
    "    return approximateDerivative\n",
    "\n",
    "def checkGradient(weights, epsilon, x, t):\n",
    "    outputLayers = calculateOutputPerLayer(weights, x)\n",
    "    y = outputLayers[1]\n",
    "    \n",
    "    # Check gradient input to hidden\n",
    "    for n in range(0, 20000, 2000):\n",
    "        print(\"For input to hidden layers\")\n",
    "        for i in range(0, 784, 200):\n",
    "            for j in range(0, 64, 20):\n",
    "                summation_k = 0\n",
    "                for k in range(len(y[0, :])):\n",
    "                    summation_k += weights[1][j][k]*(y[n][k]-t[n][k])\n",
    "                actualDerivative = outputLayers[0][n][j]*(1-outputLayers[0][n][j])*x[n][i]*(summation_k)\n",
    "                approximateDerivative = calculateApproximateDerivative(weights, epsilon, x, y, t, i, j, n, 0)\n",
    "                diff = abs(actualDerivative-approximateDerivative)\n",
    "                print(\"Difference between approximate gradient = \", diff, \" \", abs(epsilon*epsilon-diff))\n",
    "        \n",
    "        print(\"For hidden to output layers\")\n",
    "        for j in range(0, 64, 20):\n",
    "            for k in range(0, 10, 3):\n",
    "                actualDerivative = outputLayers[0][n][j]*(y[n][k]-t[n][k])\n",
    "                approximateDerivative = calculateApproximateDerivative(weights, epsilon, x, y, t, i, j, n, 1)\n",
    "                diff = abs(actualDerivative-approximateDerivative)\n",
    "                print(\"Difference between approximate gradient = \", diff, \" \", abs(epsilon*epsilon-diff))\n",
    "\n",
    "def calculateAccuracy(weights, x, t):\n",
    "    predClass = calculateOutputPerLayer(weights, x)[1]\n",
    "    predClass = YHotEncoding(predClass)\n",
    "    return accuracy_score(t, predClass)\n",
    "\n",
    "def gradientUpdateSingleUpdate(weights, x, t, learningRate):\n",
    "    outputLayers = calculateOutputPerLayer(weights, x)\n",
    "    inputSecondLayer = np.insert(outputLayers[0], 0, 1, axis=1)\n",
    "    updateWtsHiddenToOutput = updateWeightsHiddenToOutput(weights[1], x, outputLayers[1], t, inputSecondLayer, learningRate)\n",
    "    updateWtsInputToHidden = updateWeightsInputToHidden(weights[0], x, outputLayers[1], t, outputLayers[0], learningRate)\n",
    "    updatedWeights = [updateWtsInputToHidden, updateWtsHiddenToOutput]\n",
    "    return updatedWeights\n",
    "\n",
    "def gradientDescent(numItrs, weights, x, t, learningRate):\n",
    "    for itr in range(numItrs):\n",
    "        # Calculate error with the given weights\n",
    "        currTrainingError = costFunction(x[0], t[0], weights)\n",
    "        currValidationError = costFunction(x[1], t[1], weights)\n",
    "        print(\"Current validation error = \", currValidationError)\n",
    "        weights = gradientUpdateSingleUpdate(weights, x[0], t[0], learningRate)\n",
    "\n",
    "checkGradient([weightsIJ, weightsJK], 0.001, images_train, labels_train)\n",
    "weightsIJ = np.random.rand(785, 64)\n",
    "weightsJK = np.random.rand(65, 10)\n",
    "gradientDescent(1000, [weightsIJ, weightsJK], [images_train, images_validation], [labels_train, labels_validation], 0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
